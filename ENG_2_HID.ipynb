{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ENG 2 HID",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "2P-XiXo6AVvE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-IWlFpJAvhW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_sentence(w):\n",
        "    # creating a space between a word and the punctuation following it\n",
        "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
        "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
        "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "    w = re.sub(r'[\" \"]+', \" \", w)\n",
        "    w = w.rstrip().strip()\n",
        "\n",
        "    # adding a start and an end token to the sentence\n",
        "    # so that the model know when to start and stop predicting.\n",
        "    w = '<start> ' + w + ' <end>'\n",
        "    return w\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJu1TC8xDNsz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_texts = []\n",
        "target_texts = []\n",
        "target2=[]\n",
        "with open('/content/hin.txt', 'r', encoding='utf-8') as f:\n",
        "    lines = f.read().split('\\n')\n",
        "for line in lines[: min(600, len(lines) - 1)]:\n",
        "    input_text = line.split('\\t')[0]\n",
        "    target_text = line.split('\\t')[1]\n",
        "    target=line.split('\\t')[2]\n",
        "    input_texts.append(input_text)\n",
        "    target_texts.append(target_text)\n",
        "    target2.append(target)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCRamT2wE7ed",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hft-YSH2EjlU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "zippedList =  list(zip(input_texts, target_texts, target2))\n",
        "lines = pd.DataFrame(zippedList, columns = ['input' , 'output', 'waste'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gy-kFYXE57J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "38f9c78c-30fd-413d-f0f2-7a4cedd90d12"
      },
      "source": [
        "lines.head(10)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>input</th>\n",
              "      <th>output</th>\n",
              "      <th>waste</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Wow!</td>\n",
              "      <td>वाह!</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #5...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Help!</td>\n",
              "      <td>बचाओ!</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #4...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Jump.</td>\n",
              "      <td>उछलो.</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #6...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Jump.</td>\n",
              "      <td>कूदो.</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #6...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Jump.</td>\n",
              "      <td>छलांग.</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #6...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Hello!</td>\n",
              "      <td>नमस्ते।</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #3...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Hello!</td>\n",
              "      <td>नमस्कार।</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #3...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Cheers!</td>\n",
              "      <td>वाह-वाह!</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #4...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Cheers!</td>\n",
              "      <td>चियर्स!</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #4...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Got it?</td>\n",
              "      <td>समझे कि नहीं?</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #4...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     input         output                                              waste\n",
              "0     Wow!           वाह!  CC-BY 2.0 (France) Attribution: tatoeba.org #5...\n",
              "1    Help!          बचाओ!  CC-BY 2.0 (France) Attribution: tatoeba.org #4...\n",
              "2    Jump.          उछलो.  CC-BY 2.0 (France) Attribution: tatoeba.org #6...\n",
              "3    Jump.          कूदो.  CC-BY 2.0 (France) Attribution: tatoeba.org #6...\n",
              "4    Jump.         छलांग.  CC-BY 2.0 (France) Attribution: tatoeba.org #6...\n",
              "5   Hello!        नमस्ते।  CC-BY 2.0 (France) Attribution: tatoeba.org #3...\n",
              "6   Hello!       नमस्कार।  CC-BY 2.0 (France) Attribution: tatoeba.org #3...\n",
              "7  Cheers!       वाह-वाह!  CC-BY 2.0 (France) Attribution: tatoeba.org #4...\n",
              "8  Cheers!        चियर्स!  CC-BY 2.0 (France) Attribution: tatoeba.org #4...\n",
              "9  Got it?  समझे कि नहीं?  CC-BY 2.0 (France) Attribution: tatoeba.org #4..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ujzV7nzFAcP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "4beb9860-b8ab-4ce5-c8de-9c1a1e6e2ac0"
      },
      "source": [
        "lines.drop('waste', axis=1)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>input</th>\n",
              "      <th>output</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Wow!</td>\n",
              "      <td>वाह!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Help!</td>\n",
              "      <td>बचाओ!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Jump.</td>\n",
              "      <td>उछलो.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Jump.</td>\n",
              "      <td>कूदो.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Jump.</td>\n",
              "      <td>छलांग.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>595</th>\n",
              "      <td>Your dog is very big.</td>\n",
              "      <td>तुम्हारा कुत्ता बहुत बड़ा है।</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>596</th>\n",
              "      <td>Your dog is very fat.</td>\n",
              "      <td>तुम्हारा कुत्ता बहुत मोटा है।</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>597</th>\n",
              "      <td>Can I use your pencil?</td>\n",
              "      <td>मैं तुम्हारी पेनसिल इस्तेमाल कर सकता हूँ क्या?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>598</th>\n",
              "      <td>Can the rumor be true?</td>\n",
              "      <td>क्या यह अफ़वाह सच हो सकती है?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>599</th>\n",
              "      <td>Can you keep a secret?</td>\n",
              "      <td>तुम राज़ रख सकते हो क्या?</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>600 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                      input                                          output\n",
              "0                      Wow!                                            वाह!\n",
              "1                     Help!                                           बचाओ!\n",
              "2                     Jump.                                           उछलो.\n",
              "3                     Jump.                                           कूदो.\n",
              "4                     Jump.                                          छलांग.\n",
              "..                      ...                                             ...\n",
              "595   Your dog is very big.                   तुम्हारा कुत्ता बहुत बड़ा है।\n",
              "596   Your dog is very fat.                   तुम्हारा कुत्ता बहुत मोटा है।\n",
              "597  Can I use your pencil?  मैं तुम्हारी पेनसिल इस्तेमाल कर सकता हूँ क्या?\n",
              "598  Can the rumor be true?                   क्या यह अफ़वाह सच हो सकती है?\n",
              "599  Can you keep a secret?                       तुम राज़ रख सकते हो क्या?\n",
              "\n",
              "[600 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzlrPxUCGM9J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2457e287-af28-4949-e385-0ba582a8f126"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers , activations , models , preprocessing , utils\n",
        "import pandas as pd\n",
        "\n",
        "print( tf.VERSION )"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.15.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zaik_csnFWi2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "2a0853b5-be9b-4310-afa2-40583525e72a"
      },
      "source": [
        "input_lines = list()\n",
        "for line in lines.input:\n",
        "    input_lines.append( line ) \n",
        "\n",
        "tokenizer = preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts( input_lines ) \n",
        "tokenized_input_lines = tokenizer.texts_to_sequences( input_lines ) \n",
        "\n",
        "length_list = list()\n",
        "for token_seq in tokenized_input_lines:\n",
        "    length_list.append( len( token_seq ))\n",
        "max_input_length = np.array( length_list ).max()\n",
        "print( 'Input max length is {}'.format( max_input_length ))\n",
        "\n",
        "padded_input_lines = preprocessing.sequence.pad_sequences( tokenized_input_lines , maxlen=max_input_length , padding='post' )\n",
        "encoder_input_data = np.array( padded_input_lines )\n",
        "print( 'Encoder input data shape -> {}'.format( encoder_input_data.shape ))\n",
        "\n",
        "input_word_dict = tokenizer.word_index\n",
        "num_input_tokens = len( input_word_dict )+1\n",
        "print( 'Number of Input tokens = {}'.format( num_input_tokens))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input max length is 6\n",
            "Encoder input data shape -> (600, 6)\n",
            "Number of Input tokens = 583\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nA19yhfHGGyB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "e685c000-c65f-40b4-83ca-0ab9d9504c91"
      },
      "source": [
        "output_lines = list()\n",
        "for line in lines.output:\n",
        "    output_lines.append( '<START> ' + line + ' <END>' )  \n",
        "\n",
        "tokenizer = preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts( output_lines ) \n",
        "tokenized_output_lines = tokenizer.texts_to_sequences( output_lines ) \n",
        "\n",
        "length_list = list()\n",
        "for token_seq in tokenized_output_lines:\n",
        "    length_list.append( len( token_seq ))\n",
        "max_output_length = np.array( length_list ).max()\n",
        "print( 'Output max length is {}'.format( max_output_length ))\n",
        "\n",
        "padded_output_lines = preprocessing.sequence.pad_sequences( tokenized_output_lines , maxlen=max_output_length, padding='post' )\n",
        "decoder_input_data = np.array( padded_output_lines )\n",
        "print( 'Decoder input data shape -> {}'.format( decoder_input_data.shape ))\n",
        "\n",
        "output_word_dict = tokenizer.word_index\n",
        "num_output_tokens = len( output_word_dict )+1\n",
        "print( 'Number of Output tokens = {}'.format( num_output_tokens))\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output max length is 11\n",
            "Decoder input data shape -> (600, 11)\n",
            "Number of Output tokens = 773\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSm1Eo18GY_H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dbfe1d9d-1946-4eb4-834a-375cbbbcb040"
      },
      "source": [
        "decoder_target_data = list()\n",
        "for token_seq in tokenized_output_lines:\n",
        "    decoder_target_data.append( token_seq[ 1 : ] ) \n",
        "    \n",
        "padded_output_lines = preprocessing.sequence.pad_sequences( decoder_target_data , maxlen=max_output_length, padding='post' )\n",
        "onehot_output_lines = utils.to_categorical( padded_output_lines , num_output_tokens )\n",
        "decoder_target_data = np.array( onehot_output_lines )\n",
        "print( 'Decoder target data shape -> {}'.format( decoder_target_data.shape ))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoder target data shape -> (600, 11, 773)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPvK0szuGc3h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "outputId": "58d30c8d-460c-465a-ae08-f408f31e5f64"
      },
      "source": [
        "encoder_inputs = tf.keras.layers.Input(shape=( None , ))\n",
        "encoder_embedding = tf.keras.layers.Embedding( num_input_tokens, 256 , mask_zero=True ) (encoder_inputs)\n",
        "encoder_outputs , state_h , state_c = tf.keras.layers.LSTM( 256 , return_state=True , recurrent_dropout=0.2 , dropout=0.2 )( encoder_embedding )\n",
        "encoder_states = [ state_h , state_c ]\n",
        "\n",
        "decoder_inputs = tf.keras.layers.Input(shape=( None ,  ))\n",
        "decoder_embedding = tf.keras.layers.Embedding( num_output_tokens, 256 , mask_zero=True) (decoder_inputs)\n",
        "decoder_lstm = tf.keras.layers.LSTM( 256 , return_state=True , return_sequences=True , recurrent_dropout=0.2 , dropout=0.2)\n",
        "decoder_outputs , _ , _ = decoder_lstm ( decoder_embedding , initial_state=encoder_states )\n",
        "decoder_dense = tf.keras.layers.Dense( num_output_tokens , activation=tf.keras.activations.softmax ) \n",
        "output = decoder_dense ( decoder_outputs )\n",
        "\n",
        "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], output )\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(), loss='categorical_crossentropy')\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/backend.py:3994: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, None, 256)    149248      input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, None, 256)    197888      input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     [(None, 256), (None, 525312      embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   [(None, None, 256),  525312      embedding_1[0][0]                \n",
            "                                                                 lstm[0][1]                       \n",
            "                                                                 lstm[0][2]                       \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, None, 773)    198661      lstm_1[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 1,596,421\n",
            "Trainable params: 1,596,421\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Afk9RaVYGlh-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2bae05e0-af32-4bf8-f1ff-a11425f1d623"
      },
      "source": [
        "model.fit([encoder_input_data , decoder_input_data], decoder_target_data, batch_size=124, epochs=250) \n",
        "model.save( 'model.h5' )"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 600 samples\n",
            "Epoch 1/250\n",
            "600/600 [==============================] - 4s 7ms/sample - loss: 3.7811\n",
            "Epoch 2/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 3.7303\n",
            "Epoch 3/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 3.4387\n",
            "Epoch 4/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 2.7923\n",
            "Epoch 5/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 2.6997\n",
            "Epoch 6/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 2.6131\n",
            "Epoch 7/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 2.5809\n",
            "Epoch 8/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 2.5265\n",
            "Epoch 9/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 2.4892\n",
            "Epoch 10/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 2.4430\n",
            "Epoch 11/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 2.3911\n",
            "Epoch 12/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 2.3378\n",
            "Epoch 13/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 2.2847\n",
            "Epoch 14/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 2.2352\n",
            "Epoch 15/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 2.1894\n",
            "Epoch 16/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 2.1509\n",
            "Epoch 17/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 2.1191\n",
            "Epoch 18/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 2.0902\n",
            "Epoch 19/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 2.0591\n",
            "Epoch 20/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 2.0323\n",
            "Epoch 21/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 2.0059\n",
            "Epoch 22/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 1.9800\n",
            "Epoch 23/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 1.9551\n",
            "Epoch 24/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 1.9301\n",
            "Epoch 25/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 1.9032\n",
            "Epoch 26/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 1.8781\n",
            "Epoch 27/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 1.8488\n",
            "Epoch 28/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 1.8202\n",
            "Epoch 29/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 1.7920\n",
            "Epoch 30/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 1.7600\n",
            "Epoch 31/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 1.7315\n",
            "Epoch 32/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 1.7014\n",
            "Epoch 33/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 1.6667\n",
            "Epoch 34/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 1.6342\n",
            "Epoch 35/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 1.6002\n",
            "Epoch 36/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 1.5657\n",
            "Epoch 37/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 1.5308\n",
            "Epoch 38/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 1.4957\n",
            "Epoch 39/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 1.4605\n",
            "Epoch 40/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 1.4263\n",
            "Epoch 41/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 1.3903\n",
            "Epoch 42/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 1.3524\n",
            "Epoch 43/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 1.3222\n",
            "Epoch 44/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 1.2855\n",
            "Epoch 45/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 1.2530\n",
            "Epoch 46/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 1.2140\n",
            "Epoch 47/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 1.1793\n",
            "Epoch 48/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 1.1440\n",
            "Epoch 49/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 1.1110\n",
            "Epoch 50/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 1.0765\n",
            "Epoch 51/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 1.0434\n",
            "Epoch 52/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 1.0083\n",
            "Epoch 53/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.9754\n",
            "Epoch 54/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.9461\n",
            "Epoch 55/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.9131\n",
            "Epoch 56/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.8842\n",
            "Epoch 57/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.8542\n",
            "Epoch 58/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.8220\n",
            "Epoch 59/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.7948\n",
            "Epoch 60/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.7655\n",
            "Epoch 61/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.7352\n",
            "Epoch 62/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.7084\n",
            "Epoch 63/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.6806\n",
            "Epoch 64/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.6548\n",
            "Epoch 65/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.6286\n",
            "Epoch 66/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.6014\n",
            "Epoch 67/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.5772\n",
            "Epoch 68/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.5533\n",
            "Epoch 69/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.5314\n",
            "Epoch 70/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.5097\n",
            "Epoch 71/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.4906\n",
            "Epoch 72/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.4697\n",
            "Epoch 73/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.4482\n",
            "Epoch 74/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.4300\n",
            "Epoch 75/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.4101\n",
            "Epoch 76/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.3917\n",
            "Epoch 77/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.3745\n",
            "Epoch 78/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.3583\n",
            "Epoch 79/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.3432\n",
            "Epoch 80/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.3264\n",
            "Epoch 81/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.3158\n",
            "Epoch 82/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.3018\n",
            "Epoch 83/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.2869\n",
            "Epoch 84/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.2743\n",
            "Epoch 85/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.2640\n",
            "Epoch 86/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.2549\n",
            "Epoch 87/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.2434\n",
            "Epoch 88/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.2322\n",
            "Epoch 89/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.2229\n",
            "Epoch 90/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.2144\n",
            "Epoch 91/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.2078\n",
            "Epoch 92/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.1957\n",
            "Epoch 93/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.1898\n",
            "Epoch 94/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.1805\n",
            "Epoch 95/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.1745\n",
            "Epoch 96/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.1671\n",
            "Epoch 97/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.1627\n",
            "Epoch 98/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.1578\n",
            "Epoch 99/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.1510\n",
            "Epoch 100/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.1457\n",
            "Epoch 101/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.1407\n",
            "Epoch 102/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.1380\n",
            "Epoch 103/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.1307\n",
            "Epoch 104/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.1272\n",
            "Epoch 105/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.1230\n",
            "Epoch 106/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.1188\n",
            "Epoch 107/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.1158\n",
            "Epoch 108/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.1136\n",
            "Epoch 109/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.1087\n",
            "Epoch 110/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.1061\n",
            "Epoch 111/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.1029\n",
            "Epoch 112/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0997\n",
            "Epoch 113/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0971\n",
            "Epoch 114/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0952\n",
            "Epoch 115/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0927\n",
            "Epoch 116/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0914\n",
            "Epoch 117/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0887\n",
            "Epoch 118/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0865\n",
            "Epoch 119/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0837\n",
            "Epoch 120/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0823\n",
            "Epoch 121/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0809\n",
            "Epoch 122/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0778\n",
            "Epoch 123/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0775\n",
            "Epoch 124/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0750\n",
            "Epoch 125/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0732\n",
            "Epoch 126/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0728\n",
            "Epoch 127/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0709\n",
            "Epoch 128/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0696\n",
            "Epoch 129/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0696\n",
            "Epoch 130/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0673\n",
            "Epoch 131/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0657\n",
            "Epoch 132/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0646\n",
            "Epoch 133/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0632\n",
            "Epoch 134/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0620\n",
            "Epoch 135/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0619\n",
            "Epoch 136/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0605\n",
            "Epoch 137/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0588\n",
            "Epoch 138/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0577\n",
            "Epoch 139/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0574\n",
            "Epoch 140/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0565\n",
            "Epoch 141/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0554\n",
            "Epoch 142/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0550\n",
            "Epoch 143/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0541\n",
            "Epoch 144/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0535\n",
            "Epoch 145/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0523\n",
            "Epoch 146/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0522\n",
            "Epoch 147/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0518\n",
            "Epoch 148/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0508\n",
            "Epoch 149/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0500\n",
            "Epoch 150/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0500\n",
            "Epoch 151/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0489\n",
            "Epoch 152/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0488\n",
            "Epoch 153/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0476\n",
            "Epoch 154/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0468\n",
            "Epoch 155/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0468\n",
            "Epoch 156/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0459\n",
            "Epoch 157/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0462\n",
            "Epoch 158/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0450\n",
            "Epoch 159/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0447\n",
            "Epoch 160/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0447\n",
            "Epoch 161/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0432\n",
            "Epoch 162/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0438\n",
            "Epoch 163/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0428\n",
            "Epoch 164/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0429\n",
            "Epoch 165/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0418\n",
            "Epoch 166/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0420\n",
            "Epoch 167/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0415\n",
            "Epoch 168/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0410\n",
            "Epoch 169/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0410\n",
            "Epoch 170/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0407\n",
            "Epoch 171/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0395\n",
            "Epoch 172/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0391\n",
            "Epoch 173/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0391\n",
            "Epoch 174/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0385\n",
            "Epoch 175/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0382\n",
            "Epoch 176/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0386\n",
            "Epoch 177/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0385\n",
            "Epoch 178/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0388\n",
            "Epoch 179/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0379\n",
            "Epoch 180/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0373\n",
            "Epoch 181/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0367\n",
            "Epoch 182/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0359\n",
            "Epoch 183/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0373\n",
            "Epoch 184/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0369\n",
            "Epoch 185/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0368\n",
            "Epoch 186/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0357\n",
            "Epoch 187/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0354\n",
            "Epoch 188/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0354\n",
            "Epoch 189/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0347\n",
            "Epoch 190/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0352\n",
            "Epoch 191/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0355\n",
            "Epoch 192/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0348\n",
            "Epoch 193/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0341\n",
            "Epoch 194/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0338\n",
            "Epoch 195/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0340\n",
            "Epoch 196/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0334\n",
            "Epoch 197/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0328\n",
            "Epoch 198/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0336\n",
            "Epoch 199/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0327\n",
            "Epoch 200/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0333\n",
            "Epoch 201/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0326\n",
            "Epoch 202/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0324\n",
            "Epoch 203/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0326\n",
            "Epoch 204/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0327\n",
            "Epoch 205/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0321\n",
            "Epoch 206/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0322\n",
            "Epoch 207/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0324\n",
            "Epoch 208/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0316\n",
            "Epoch 209/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0322\n",
            "Epoch 210/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0317\n",
            "Epoch 211/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0304\n",
            "Epoch 212/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0311\n",
            "Epoch 213/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0310\n",
            "Epoch 214/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0311\n",
            "Epoch 215/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0299\n",
            "Epoch 216/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0310\n",
            "Epoch 217/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0308\n",
            "Epoch 218/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0306\n",
            "Epoch 219/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0304\n",
            "Epoch 220/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0300\n",
            "Epoch 221/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0300\n",
            "Epoch 222/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0302\n",
            "Epoch 223/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0293\n",
            "Epoch 224/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0295\n",
            "Epoch 225/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0296\n",
            "Epoch 226/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0291\n",
            "Epoch 227/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0294\n",
            "Epoch 228/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0284\n",
            "Epoch 229/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0298\n",
            "Epoch 230/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0287\n",
            "Epoch 231/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0295\n",
            "Epoch 232/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0288\n",
            "Epoch 233/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0286\n",
            "Epoch 234/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0289\n",
            "Epoch 235/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0293\n",
            "Epoch 236/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0285\n",
            "Epoch 237/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0283\n",
            "Epoch 238/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0285\n",
            "Epoch 239/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0277\n",
            "Epoch 240/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0279\n",
            "Epoch 241/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0284\n",
            "Epoch 242/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0278\n",
            "Epoch 243/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0279\n",
            "Epoch 244/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0279\n",
            "Epoch 245/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0270\n",
            "Epoch 246/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0273\n",
            "Epoch 247/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0277\n",
            "Epoch 248/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0276\n",
            "Epoch 249/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0276\n",
            "Epoch 250/250\n",
            "600/600 [==============================] - 2s 3ms/sample - loss: 0.0279\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gL2U6j_hGp41",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_inference_models():\n",
        "    \n",
        "    encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n",
        "    \n",
        "    decoder_state_input_h = tf.keras.layers.Input(shape=(256,))\n",
        "    decoder_state_input_c = tf.keras.layers.Input(shape=(256,))\n",
        "    \n",
        "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "    \n",
        "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
        "        decoder_embedding , initial_state=decoder_states_inputs)\n",
        "    decoder_states = [state_h, state_c]\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "    decoder_model = tf.keras.models.Model(\n",
        "        [decoder_inputs] + decoder_states_inputs,\n",
        "        [decoder_outputs] + decoder_states)\n",
        "    \n",
        "    return encoder_model , decoder_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozH6mFxRIgvY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def str_to_tokens( sentence : str ):\n",
        "    words = sentence.lower().split()\n",
        "    tokens_list = list()\n",
        "    for word in words:\n",
        "        tokens_list.append( input_word_dict[ word ] ) \n",
        "    return preprocessing.sequence.pad_sequences( [tokens_list] , maxlen=max_input_length , padding='post')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2a_lAhcIj1J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 767
        },
        "outputId": "c61abd4f-646b-4b6a-f963-a5554969035c"
      },
      "source": [
        "enc_model , dec_model = make_inference_models()\n",
        "\n",
        "enc_model.save( 'enc_model.h5' ) \n",
        "dec_model.save( 'dec_model.h5' ) \n",
        "model.save( 'model.h5' ) \n",
        "\n",
        "for epoch in range( encoder_input_data.shape[0] ):\n",
        "    states_values = enc_model.predict( str_to_tokens( input( 'User: ' ) ) )\n",
        "    empty_target_seq = np.zeros( ( 1 , 1 ) )\n",
        "    empty_target_seq[0, 0] = output_word_dict['start']\n",
        "    stop_condition = False\n",
        "    decoded_translation = ''\n",
        "    while not stop_condition :\n",
        "        dec_outputs , h , c = dec_model.predict([ empty_target_seq ] + states_values )\n",
        "        sampled_word_index = np.argmax( dec_outputs[0, -1, :] )\n",
        "        sampled_word = None\n",
        "        for word , index in output_word_dict.items() :\n",
        "            if sampled_word_index == index :\n",
        "                decoded_translation += ' {}'.format( word )\n",
        "                sampled_word = word\n",
        "        \n",
        "        if sampled_word == 'end' or len(decoded_translation.split()) > max_output_length:\n",
        "            stop_condition = True\n",
        "            \n",
        "        empty_target_seq = np.zeros( ( 1 , 1 ) )  \n",
        "        empty_target_seq[ 0 , 0 ] = sampled_word_index\n",
        "        states_values = [ h , c ] \n",
        "\n",
        "    print( \"Bot:\" +decoded_translation.replace(' end', '') )\n",
        "    print()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "User: wow\n",
            "Bot: वाह\n",
            "\n",
            "User: jump\n",
            "Bot: कूदो\n",
            "\n",
            "User: i love you\n",
            "Bot: मैं तुमसे प्यार करती हूँ।\n",
            "\n",
            "User: i can swim\n",
            "Bot: मुझे तैरना आता है।\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    729\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    465\u001b[0m         \"\"\"\n\u001b[0;32m--> 466\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-ab74e0595754>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mencoder_input_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mstates_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mstr_to_tokens\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m'User: '\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mempty_target_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m(\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mempty_target_seq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_word_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'start'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m         )\n\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    733\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 735\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    736\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAO0biDjIpyt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}